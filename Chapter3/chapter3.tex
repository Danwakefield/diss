\chapter{Implementation}

%The implementation should look at any issues you encountered as you tried to implement your design. During the work, you might have found that elements of your design were unnecessary or overly complex; perhaps third party libraries were available that simplified some of the functions that you intended to implement. If things were easier in some areas, then how did you adapt your project to take account of your findings?

%It is more likely that things were more complex than you first thought. In particular, were there any problems or difficulties that you found during implementation that you had to address? Did such problems simply delay you or were they more significant? 

%You can conclude this section by reviewing the end of the implementation stage against the planned requirements. 

\section{Arithmetic}
The arithmetic construct was the first thing I created.
It is a standalone piece of code only exposing a single method, Parse, which takes a string representing an equation and returning the integer value it evaluates to or an error.
Because of this isolation I created it as a subpackage which also allowed me to simplify the error handling.

I was able to panic and recover\footnote{Panic is similar to an exception in other languages but has different semantics. Errors in Go should be passed through the use of multiple return values instead.} to completely unwind the parser and lexer.
Although panics are reserved for truly exceptional cases in Go, they had to be used in this case.
Errors encountered during lexing or parsing of a language are almost always fatal as they leave the system in an indeterminate state.
For example, the following make no sense in the context of a shell mathematical equation:
\begin{verbatim}
++
  or
35 67 89
\end{verbatim}
This is an implementation detail and can of course be treated differently.
In bash the '++' symbol can represent post-increment or pre-increment depending on its position.
The numbers could also be joined together into a single literal; Python and Java do something similar with underscores rather than spaces\cite{UNDERSCORE-NUM-LITERAL}.

Another of Go's golden rules is never to raise a panic across package boundaries and to do this we defer a recover function.
If recover detects we have panicked it checks to see if it should be returned as an error instead.
Unfortunately during testing I discovered that the checks where not through enough and allowed division by zero errors to bubble up crashing the whole program. 
The fix for this would be quite simple, it just requires the addition of a division helper function similar to that used for shifting (See \ref{lst:arith-shift}). XXX % Add reference to integration test that caught it.
\newpage %XXX NEWPAGE 
\subsection{Lexer}
The Lexer was completed during the first iteration as planned and has a good set of unit tests that where used to ensure the correctness of returned tokens.

The syntax for an equation is quite simple, consisting of just:
\begin{itemize*}
	\item Variable Names
    \item Symbols
	\item Numeric Literals
\end{itemize*}

Variable names are detected using the following simple regular expression, \verb![a-zA-Z_][a-zA-Z0-9_]*!.

POSIX requires that detection of numeric values can be done in base 8 (Octal), 10 (Decimal) and 16 (Hexadecimal).
Values are always converted to base 10 for use inside the lexer.
If the literal is invalid, e.g \verb!0xffk! an error is returned that unwinds the parser.
With the current design if assignments have been applied before the error they would remain in effect, see~\ref{sec:scope}.

Symbols are the simplest thing to detect, as the code in~\ref{lst:arith-symbol-lex} shows, they only require one character of lookahead.

\subsection{Parser}
The parser was not completed along with the lexer in the first iteration as had been planned.
It was finished in the second one along with extensions to Scope.

The tokens passed from the lexer are assigned to their node type by a simple switch statement.
The node types are used to abstract the functionality of similar operations.
For example $a + b$, $a * b$ and $a \mathrel{+}= b$, $a \mathrel{*}= b$ show that behaviour is similar and it is just the operation that differs in most cases .
Respectively they would be created as InfixNode and InfixAssignNode with their Types set to indicate what operation is desired.

The Pratt parser requires that each node have two functions and one value, see~\ref{lst:arith-node-interface}.
Since Go interfaces can only consist of methods I just changed the value to a getter method instead.

\subsection{Variables}
To begin with I created stub methods on the parser that would eventually interact with the supplied scope.

Values are converted to strings before being stored.
When they are retrieved if they can be parsed as any of octal, decimal or hex they are converted before being returned.
Variables that cannot be parsed are returned with a value of zero.

Variables can only be accessed by using their unprefixed form, i.e \verb!(( a = 5 ))! will work but \verb!(( $a = 5 ))! will not.
This is because variable expansion is done separately and before arithmetic expansion.
Time constraints and an initially flawed lexer design means that this does not currently work(See~\ref{sec:main-lexer-arith}.

\subsection{Ternary}
The ternary operator is unique in that it operates on three values.
Unfortunately due to the fact that the tree is constructed and then evaluated immediately this led to a bug.

When both sides of the environment contained assignment operators variables could be modified twice or always assigned the second value, E.g given \verb!y = 0! an equation like \\ \verb!x ? y += 1 : y += 3! would make \verb!y == 4! no matter what x was.

I left the bug with a failing test case but went back to fix it in the last iteration using the knowledge I had gained during the project.

\section{Scope}
\label{sec:scope}
The scope object was created during the second iteration alongside the arith parser.
It was also created as a subpackage because putting it in the main package was impossible since Go completely disallows circular references.

As is obvious from the package name, 'variables.Scope' and the headings in this section, the responsibilities of the class grew quickly and it quickly became mislabeled.
I realised the problems that had been created by the package layout
as I started to implement user functions in the penultimate iteration.
At this point it would have taken significant work to correct and the benefits would have been minimal.

\subsection{Variables}
The original Scope object was only used to get and set variables but it has lots of complexity.
It contains a list of hash tables which are searched in reverse order when setting, updating or retrieving anything.
The first value matching the name is used.

I created it this way as I knew that it would be needed by both commands with temporary assignments and functions using the local builtin.

Commands take a list of environment variables each as a string in the form, "VARNAME=FOO".
This meant that when I got to the stage when commands where being executed I had to add an Environ function to get them in the correct form.
The complexity of this function is O(2n), where n is the number of variables set in the shell, but I could not see a way to improve this without harming the performance of other aspects.

The design does not currently support integer typed variables but It could be added with minimal effort.
Change the Variable struct to contain an isInteger bool and when updating this variable using the result of arith.Parse as the new value. 
\subsection{User Functions}
User functions where added in the penultimate iteration.
The way they are stored in the Scope object shows how I was bitten by lack of forethought area of package layout.

\begin{verbatim}
type Scope struct {
	...
	Functions    map[string]interface{}
	...
}
\end{verbatim}

It shows that I have had to use a map of interface\{\} to store the NodeFunctions.
It is similar to a \verb!void *! in C as Go's implicit satisfaction of interfaces mean that everything satisfies the empty one.

This was done as storage and usage of a NodeFunction is done during the Eval phase which is defined in package main.
The solution would have been to create a subpackage for nodes.
However the work required to change the parser during the last week was greater than the bad practice of using interface\{\} here.

\subsection{Aliases}
Aliases where finished in the time frames.
They will however require storage in the Scope object and use of a builtin command to put them there.

The way the main lexer is created will also have to be different as aliases are expanded before anything is parsed but only when they are known about.

They also only take effect after after the compound block they are declared in is evaluated.
\begin{verbatim}
foofunc() {
    foo "IT WORKS"
}

alias foo=echo
foofunc
\end{verbatim}
This would cause the shell to print an error as the alias did not exist before the function was parsed.
foo will be available for the rest of the script but the foofunc function will always cause an error.

\section{Main Lexer}
The main lexer is a coroutine based Non-Deterministic Finite Automata(NDFA).
I used this design as the code for the Go template parser was done in the same style.
The presentation given by Rob Pike\cite{PIKE-LEXING-VIDEO} also made it seem very simple.

A concurrent run loop is started when the lexer is created which means that on machines with multiple cores (Almost everything modern) this design will be a performance boost.
During this run loop state functions are called, beginning with lexStart, which either return another state function or nil to indicate that parsing has completed. 
Each lexing state has a minimal amount of responsibilities deferring to others when possible.

An example of how the lexer works is below.
Given the text \verb!"abc $def"!.
lexStart will pass control to the lexDoubleQuote function when it see's the '"' character which will pass control to the lexSubstitution when it see's '\$'.
This will determine that lexSimpleVariable is needed as neither '{' or '(' follow the dollar.
Once that completes control is returned to lexDoubleQuote which will see the closing quote, emit the token and return control to lexStart.

The emit function takes a token type and constructs a lexitem, sometime know as a lexeme, that contains the required information to be parsed and evaluated successfully.
This includes its line in the source, the string representation, any substitutions it contains and if the string should be treated as quoted
The values in the lexer that hold this information are cleared when an emit occurs.

For a simpler syntax this design is perfect but it turned out to be quite bad for a shells.
They are some times in the next section when creating a new lexer and using its output is mentioned.
It is because the lexer is not re-entrant that a new one must be created whenever we require tokens while in the process of creating one.
I am uncertain if this workaround suffers from memory leaks.
As the lexer runs concurrently there is a reference to it in the created thread which may prevent the garbage collector from collecting it.

\subsection{Words}
The word lexing code was completed during the third iteration along with the stubbed parser.

Word are anything that is not quoted, a variable or a subshell.
This almost always means that

\subsection{Strings}
Two functions are used for strings as the shells has both raw quotes and interpreted quotes.

Raw quotes use the single quote character and are very simple.
Every character is copied as is except the EOF and the matching single quote.
EOF causes an unterminated string error.
The end quote causes the string to be emitted.

Interpreted strings, which use double quotes, should allow variables, both types of subshells and escape sequences.
Unfortunately escape sequences where not completed so the string '\\n' will be treated as two separate character rather than a newline.
The position of subshells and variables in a string are indicated by the SentinalSubstitution character.
The subs list of each lexitem holds the structures that can be called to replace these sentinals.
This is done to abstract to complexities of substitutions.
Interpreted strings also throw an error when encountering EOF.

When they where created during the third iteration both single and double quotes behaved as raw strings.
As the capabilities of the lexer improved the interpreted string function could add a new rule to its switch statement and
pass control to incorporate them.

\subsection{Variables}
Variables where completed in two stages.
Simple ones in the fourth iteration alongside extensions to the parser.
Complex variables during the XXX.

The lexer has a lot of knowledge about variables due to the concurrent design and the interpreted nature of strings.
This would be better suited in the parser and it would also fix the problems that exist with more complex substitutions.
A complex variable substitution of the form \verb!${foo:-$bar}! does not currently work.

This shows a variable 'foo' being operated on by the ':-' operator.
When I created the lexComplexVariable function I used a naive lexer for anything after the operator with the intention of replacing it.
It gathers everything as is upto the next '}' character and uses it as the value.
Creating another lexer using the remaining input and extracting tokens would be the solution in the current state.
However redesigning the lexer to be recursive descent would make it even easier.

XXX

While writing this section and inspecting the code I also realised the naive collection does not check for the EOF character.
This means that a memory based DOS attack is possible by just using a file with the contents \verb!${a:!.

\subsection{Subshells}
The original form uses grave quotes as delimiters and the 'modern' version uses '\$( command )'.
They work similarly to user defined functions but output is captured instead of being output to the terminal.
Changes to the environment that take place inside a subshell are not propagated. 

For the modern version when the start of a subshell is detected a new lexer and parser pair is created with the remaining input.
This parser is started with IgnoreNewlines option and returns an AST of everything contained in the the brackets.
The lexer on the current level then adjusts its position in the input to the character after the closing bracket.
This works quite well but the same thing cannot be used for the old style.

\begin{verbatim}
$( foo $( bar $( baz ) ) )  # Modern

`foo \` bar \\\`baz\\\` \` `  # Old
\end{verbatim}
The code above shows how nesting three levels works using both type of subshell.
Parsing an old style subshell first requires that you find the correct ending grave, I.e not escaped or in a string.
Then you have to scan through the string and for every grave that is not quoted remove a level of nesting.
The first level uses a single backslash to escape and each subsequent level uses an additional two.
The result of this modification can then be passed into a new parser and the process continued.

Once parsed both styles are represented identically in the AST.
Because of this I focused on implementing the modern style as a proof of concept.

\subsection{Arithmetic} \label{sec:main-lexer-arith}
As the heavy lifting is done by its own lexer and parser we only have to gather the contents to pass along.
The main lexer gained the ability to recognize them during the forth iteration along with simple variables.

This is done by counting the number of parentheses that are encountered while stepping though the input.
We finish lexing when we see two consecutive righ
t brackets while we are not in a bracketed expression.
I.e \verb!$(( 2+4 ) ) ))! will lex identically to \verb!$(( 2 + 4 ))! as right brackets are ignored when they are not consecutive or part of an expression.
Everything between the two markers is stored in a substitution object to be passed to 'arith.Parse'.

In the specification this construct is expanded after applying other substitutions.
E.g \verb!$(( 2 + $(echo 4) ))! should reduce to \verb!$(( 2 + 4 ))! and then be evaluated as math.
Again this was not done because of the non re-entrant nature of the lexer.

\section{Main Parser, AST \& Nodes}

\subsection{Simple Command}
\subsection{Command}
\subsection{Pipeline}
\subsection{And Or}
\subsection{List}

\section{Expansion}

\subsection{Tilde}
\subsection{Substitutions}
\subsection{Globbing}
\subsection{Word Splitting}

\section{Builtin Commands}
The last iteration included built-in commands.



\section{Circular References}
\label{sec:circular-refs}





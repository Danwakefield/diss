\section{Main Lexer}
The main lexer is a coroutine based Non-Deterministic Finite Automata(NDFA).
The inspired to use this design by Rob Pike's presentation of the Go template parser\cite{PIKE-LEXING-VIDEO}.
The design is simple and made it very easy to follow along with the path of the code.

A concurrent run loop is started when the lexer is created which means that when run on machines with multiple cores (Almost everything modern) performance will be slightly improved.
During this run loop state functions are called, beginning with lexStart, which either return another state function or nil to indicate that lexing has completed. 
Each state defers to others when possible to reduce code duplication.

Given the text \verb!"abc $def"!. lexStart will pass control to the lexDoubleQuote function when it see's the '"' character which will pass control to the lexSubstitution when it see's '\$'.
This will determine that lexSimpleVariable is needed as neither '\{' or '(' follow the dollar sign.
Once that completes control is returned to lexDoubleQuote which will see the closing quote, emit a TokenWord which contains the contents of the string, and finally return control to lexStart.

The emit function takes a token type and constructs a lexitem, sometime know as a lexeme, that contains the required information to be parsed and evaluated successfully.
This information included is the line and position of the token in the source, the string representation, if the string should be treated as quoted and substitution structs.
Substitution structs hold information on how to replace interpolated values, See~\ref{sec:substitution}.
The values in the lexer that hold this information are reset when an emit occurs.

For a simpler syntax this design is perfect but it turned out to be quite bad for a shells.
They are some times in the next section when creating a new lexer and using its output is mentioned.
It is because the lexer is not re-entrant that a new one must be created whenever we require tokens while in the process of creating one.
I am uncertain if this workaround suffers from memory leaks.
As the lexer runs concurrently there is a reference to it in the created thread which may prevent the garbage collector from collecting it.

\subsection{Words}
The word lexing code was completed during the third iteration along with the stubbed parser.

Word are anything that is not quoted, a variable or a subshell.
This almost always means that

XXX

\subsection{Strings}
Two functions are used for strings as the shells has both raw quotes and interpreted quotes.

Raw quotes use the single quote character and are very simple.
Every character is copied as is except the EOF and the matching single quote.
EOF causes an unterminated string error.
The end quote causes the string to be emitted.

Interpreted strings, which use double quotes, should allow variables, both types of subshells and escape sequences.
Unfortunately escape sequences where not completed so the string '\\n' will be treated as two separate character rather than a newline.
The position of subshells and variables in a string are indicated by the SentinalSubstitution character.
The subs list of each lexitem holds the structures that can be called to replace these sentinals.
This is done to abstract to complexities of substitutions.
Interpreted strings also throw an error when encountering EOF.

When they where created during the third iteration both single and double quotes behaved as raw strings.
As the capabilities of the lexer improved the interpreted string function could add a new rule to its switch statement and
pass control to incorporate them.

\subsection{Variables}
Variables where completed in two stages.
Simple ones in the fourth iteration alongside extensions to the parser.
Complex variables during the XXX.

The lexer has a lot of knowledge about variables due to the concurrent design and the interpreted nature of strings.
This would be better suited in the parser and it would also fix the problems that exist with more complex substitutions.
A complex variable substitution of the form \verb!${foo:-$bar}! does not currently work.

This shows a variable 'foo' being operated on by the ':-' operator.
When I created the lexComplexVariable function I used a naive lexer for anything after the operator with the intention of replacing it.
It gathers everything as is upto the next '\}' character and uses it as the value.
Creating another lexer using the remaining input and extracting tokens would be the solution in the current state.
However redesigning the lexer to be recursive descent would make it even easier.

XXX

While writing this section and inspecting the code I also realised the naive collection does not check for the EOF character.
This means that a memory based DOS attack is possible by just using a file with the contents \verb!${a-!.

\subsection{Subshells}
The original form uses grave quotes as delimiters and the 'modern' version uses '\$( command )'.
They work similarly to user defined functions but output is captured instead of being output to the terminal.
Changes to the environment that take place inside a subshell are not propagated. 

For the modern version when the start of a subshell is detected a new lexer and parser pair is created with the remaining input.
This parser is started with IgnoreNewlines option and returns an AST of everything contained in the the brackets.
The lexer on the current level then adjusts its position in the input to the character after the closing bracket.
This works quite well but the same thing cannot be used for the old style.

\begin{verbatim}
$( foo $( bar $( baz ) ) )  # Modern

`foo \` bar \\\`baz\\\` \` `  # Old
\end{verbatim}
The code above shows how nesting three levels works using both type of subshell.
Parsing an old style subshell first requires that you find the correct ending grave, I.e not escaped or in a string.
Then you have to scan through the string and for every grave that is not quoted remove a level of nesting.
The first level uses a single backslash to escape and each subsequent level uses an additional two.
The result of this modification can then be passed into a new parser and the process continued.

Once parsed both styles are represented identically in the AST.
Because of this I focused on implementing the modern style as a proof of concept.
XXX

\subsection{Arithmetic} \label{sec:main-lexer-arith}
As the heavy lifting is done by its own lexer and parser we only have to gather the contents to pass along.
The main lexer gained the ability to recognize them during the forth iteration along with simple variables.

This is done by counting the number of parentheses that are encountered while stepping though the input.
Lexing finishes when two consecutive right brackets that are part of a bracketed expression are seen.
I.e \verb!$(( 2+4 ) ) ))! will lex identically to \verb!$(( 2 + 4 ))! as right brackets are ignored when they are not consecutive or part of an expression.
Everything between the two markers is stored in a substitution object to be passed to 'arith.Parse'.

In the specification substitutions contained within this construct are expanded before itself.
E.g \verb!$(( 2 + $(echo 4) ))! should reduce to \verb!$(( 2 + 4 ))! and then be evaluated as math.
Again this was not done because of the non re-entrant nature of the lexer. XXX













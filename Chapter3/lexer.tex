\section{Main Lexer}
The main lexer is a coroutine based Non-Deterministic Finite Automata(NDFA).
I was inspired to use this design by Rob Pike's presentation of Go's template parser\cite{PIKE-LEXING-VIDEO}.
The code in the standard library is quite simple and is easy to follow along with.

A concurrent run loop is started when the lexer is created which means that when run on machines with multiple cores (Almost everything modern) performance will be slightly improved.
During this run loop state functions are called, beginning with lexStart, which either return another state function or nil to indicate that lexing has completed. 
Each state defers to others when possible to reduce code duplication.

Given the text \verb!"abc $def"!. lexStart will pass control to the lexDoubleQuote function when it see's the '"' character which will pass control to the lexSubstitution when it see's '\$'.
This will determine that lexSimpleVariable is needed as neither '\{' or '(' follow the dollar sign.
Once that completes control is returned to lexDoubleQuote which will see the closing quote, emit a TokenWord which contains the contents of the string, and finally return control to lexStart.

The emit function takes a token type and constructs a lexitem, sometime know as a lexeme, that contains the required information to be parsed and evaluated successfully.
This information includes the line number and position of the token in the source, its string representation, if the string should be treated as quoted and substitution structs.
Substitution structs hold information on how to replace interpolated values, See~\ref{sec:substitution}.
The values in the lexer that hold this information are reset when an emit occurs.

For a simpler syntax this design is perfect but it turned out to be unsuited to the shell.
They are some times in the next section when creating a new lexer and using its output is mentioned.
Because of the concurrent nature of the lexer this is the only way to safely get tokens while in the middle of lexing another portion of the input.
This workaround may also suffer from memory leaks as there is a reference to the lexer in the go-routine thread which might prevent the garbage collector from collecting it.

Tokens returned from the lexer are one of three types.

Word tokens which are strings or unquoted text not recognized as a keyword.

Control tokens which represent keywords such as 'for'.
Detection of these tokens is disabled by the parser in certain cases.
For example the following is allowed \verb!for for in for; do for=for; done; echo $for! as both the second and third for are parsed with the flag to disable keyword detection.

The final types are operators like EOF and Newline. 
Newlines can also be suppressed during the lexing which will then continue until a non Newline token is found. 

\subsection{Words}
The word lexing code was completed during the third iteration along with the stubbed parser.

Words are any text that is not a control operator, symbol or substitution.
They contain a field that allows the parser to check if it is a string.
This information will eventually be used to determine the expansions and word splits to perform on it.

\subsubsection{Strings}
Two functions are used for strings as the shells has both raw quotes and interpreted quotes.

Raw quotes use the single quote character and are very simple.
Every character is copied as is except the EOF and the matching single quote.
EOF causes an unterminated string error.
The end quote causes the string to be emitted.

Interpreted strings, which use double quotes, should allow variables, both types of subshells and escape sequences.
Unfortunately escape sequences where not completed so the string '\\n' will be treated as two separate character rather than a newline.
The position of subshells and variables in a string are indicated by the SentinalSubstitution character.
The subs list of each lexitem holds the structures that can be called to replace these sentinals.
This is done to abstract to complexities of substitutions.
Interpreted strings also throw an error when encountering EOF.

When they where created during the third iteration both single and double quotes behaved as raw strings.
As the capabilities of the lexer improved the lexDoubleQuote function could add a new rule to its switch statement and pass control to incorporate them.

\subsection{Variables}
Variables where completed in two stages.
Simple ones in the fourth iteration alongside extensions to the parser.
Complex variables during the XXX.

The lexer has a lot of knowledge about variables due to the concurrent design and the interpreted nature of strings.
This would be better suited in the parser and it would also fix the problems that exist with more complex substitutions.
A complex variable substitution of the form \verb!${foo:-$bar}! does not currently work.

This shows a variable 'foo' being operated on by the ':-' operator.
When I created the lexComplexVariable function I used a naive lexer for anything after the operator with the intention of replacing it.
It gathers everything as is upto the next '\}' character and uses it as the value.
Creating another lexer using the remaining input and extracting tokens would be the solution in the current state.
However redesigning the lexer to be recursive descent would make it even easier.

\subsubsection{Variable Lexing Bug.}
While writing this section and inspecting the code I noticed that the naive collection does not check for the EOF character.
This means that a memory based DOS attack is possible by just using a file with the contents \verb!${a-!.
The lexing code will continue appending the EOF character to the buffer as calls to 'next' continuously return it once it has been reached.

The previously mentioned design improvements would have removed this bug.
A fix in the meantime would be to simply panic if the EOF character is seen.
An unterminated variable is not defined behaviour and while a panic will not be the most descriptive way of indicating the error it would stop resource abuse.

\subsection{Subshells}
The original form uses grave quotes as delimiters and the 'modern' version uses '\$( command )'.
They work similarly to user defined functions but output is captured instead of being output to the terminal.
Changes to the environment that take place inside a subshell are not propagated. 

For the modern version when the start of a subshell is detected a new lexer and parser pair is created with the remaining input.
This parser is started with IgnoreNewlines option and returns an AST of everything contained in the the brackets.
The lexer on the current level then adjusts its position in the input to the character after the closing bracket.
This works quite well but the same thing cannot be used for the old style.

\begin{verbatim}
$( foo $( bar $( baz ) ) )  # Modern

`foo \` bar \\\`baz\\\` \` `  # Old
\end{verbatim}
The code above shows how nesting three levels works using both type of subshell.
Parsing an old style subshell first requires that you find the correct ending grave, I.e not escaped or in a string.
Then you have to scan through the string and for every grave that is not quoted remove a level of nesting.
The first level uses a single backslash to escape and each subsequent level uses an additional two.
The result of this modification can then be passed into a new parser and the process repeated.

Once parsed both styles are represented identically in the AST.
Because of this I focused on implementing the modern style as the proof of concept.



\subsection{Arithmetic} \label{sec:main-lexer-arith}
As the heavy lifting is done by its own lexer and parser we only have to gather the contents to pass along.
The main lexer gained the ability to recognize them during the forth iteration along with simple variables.

This is done by counting the number of parentheses that are encountered while stepping though the input.
Lexing finishes when two consecutive right brackets that are part of a bracketed expression are seen.
I.e \verb!$(( 2+4 ) ) ))! will lex identically to \verb!$(( 2 + 4 ))! as right brackets are ignored when they are not consecutive or part of an expression.
Everything between the two markers is stored in a substitution object to be passed to 'arith.Parse'.

In the specification substitutions contained within this construct are expanded before itself.
E.g \verb!$(( 2 + $(echo 4) ))! should reduce to \verb!$(( 2 + 4 ))! and then be evaluated as math.
Again this was not done because of the non re-entrant nature of the lexer. XXX













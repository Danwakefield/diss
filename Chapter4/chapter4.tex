\chapter{Testing}

%Detailed descriptions of every test case are definitely not what is required here. What is important is to show that you adopted a sensible strategy that was, in principle, capable of testing the system adequately even if you did not have the time to test the system fully.

%Have you tested your system on "real users"? For example, if your system is supposed to solve a problem for a business, then it would be appropriate to present your approach to involve the users in the testing process and to record the results that you obtained. Depending on the level of detail, it is likely that you would put any detailed results in an appendix.

%the following sections indicate some areas you might include. Other sections may be more appropriate to your project. 

\section{Overall Approach to Testing}
This project took a detailed approach to testing due to the development process including TDD from the start and evolving into FDD when a more concrete product appeared.
This paid off multiple times catching bugs and regressions that occurred.

TDD was used throughout but focus switched from unit tests to integration tests as the complexity of test cases grew. 
The large size of the AST's produced for even simple statements (See~\ref{lst:if-ast-2}) meant that this swap saved significant time.

\section{Unit Tests}
Go comes with a builtin testing package as well as test runner tools to encourage testing from the beginning.
Tests can be placed in the same package as the code or they can use a special package ('X\_test' with X replaced by the package name) that only allows access to exported values.
This is a very simple way to ensure that any testing is blackbox and can help find places where package boundaries are too strict or weak.

Tests in Go are almost always table driven (See~\ref{lst:parser-unit-test} for an example).
This is the practice of testing with sets of inputs and comparing against the expected outputs.
The values used in these tests range from edge cases to common usage and are designed to stress the code.

\section{Integration Testing}
The integration testing for the project was done with shell scripts and golden files.
These tests exercised portions of shell syntax and output strings indicating success or failure.

Before each feature was developed a script containing actions it should be able to accomplish was created.
This file was tested by running it in the dash shell and observing if it was successful.
This was part of the test first mantra of TDD and having a second implementation that could prove the shell scripts worked as intended was very helpful.

Golden files containing this proven correct output where created.
A helper script that rebuilt the program and ran each test file against its corresponding golden file was created.
Running the tests after each change could then be easily show when the implementation of a feature was complete and correct.

\section{Automated Testing}
During my project I used Github and a hosted continuous integrations service called drone.io\footnote{My project's public page is available at \url{https://drone.io/github.com/Danwakefield/gosh}}.
Every time a commit was pushed to the repository the service would clone the code and run both the unit and integration tests against the changes.
Notification emails were sent with the status of the build.

Developing as a single person team this was only partly helpful.
The system ran no extra tests just the ones that would be run locally.
However they are run in a container system which means that destructive regressions or malicious patches can be caught before they affect anyone's real system. 
CI systems also make it easy for any future contributors to see the status of the code and they show that code quality and reliability is considered.














